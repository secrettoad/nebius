apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-model
  labels:
    app: llm-model
spec:
  selector:
    matchLabels:
      app: llm-model
  template:
    metadata:
      labels:
        app: llm-model
    spec:
      containers:
        - name: llm-model
          image: jhurdle/nebius-demo:3  # Replace with your image
          ports:
            - containerPort: 8080  # Adjust based on your model's API
          resources:
            limits:
              memory: "180Gi"
              cpu: "15"
            requests:
              memory: "180Gi"
              cpu: "15"
          volumeMounts:
            - name: model-storage
              mountPath: /models
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      volumes:
        - name: model-storage
          emptyDir: {}  # Replace with a PersistentVolumeClaim if needed
---
apiVersion: v1
kind: Service
metadata:
  name: llm-service
spec:
  selector:
    app: llm-model
  ports:
    - protocol: TCP
      port: 80  # External port
      targetPort: 8080  # Match containerPort in Deployment
  type: LoadBalancer  # Use NodePort if not on a cloud provider
