import sys

import torch
import pytorch_lightning as pl
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import Dataset, DataLoader, ConcatDataset
import json
from pytorch_lightning.utilities.rank_zero import rank_zero_only
import os

os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
# Tokenizer
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Ensure padding

sys.stderr.write('num gpus available: %'.format(torch.cuda.device_count()))
# Load dataset


class JSONLinesDataset(Dataset):
    def __init__(self, file_path, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []

        with open(file_path, "r") as f:
            for i, line in enumerate(f):
                if i > 200:
                    break
                json_obj = json.loads(line)
                text = json_obj["text"]  # Adjust key as needed
                tokenized = tokenizer(text, truncation=True, max_length=self.max_length, padding="max_length", return_tensors="pt")
                input_ids = tokenized["input_ids"].squeeze(0)
                attention_mask = tokenized["attention_mask"].squeeze(0)  # Attention mask generated by tokenizer

                # Labels are the same as input_ids but shifted left internally by the model
                labels = input_ids.clone()

                # Store input_ids, labels, and attention_mask
                self.data.append({
                    "input_ids": input_ids,
                    "labels": labels,
                    "attention_mask": attention_mask
                })

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


# Collate function for DataLoader
def collate_fn(batch):
    return {key: torch.stack([item[key] for item in batch]) for key in batch[0]}


class LawDataModule(pl.LightningDataModule):
    def __init__(self, batch_size=4):
        super().__init__()
        self.batch_size = batch_size

    def setup(self, stage=None):
        # Load individual datasets with tokenizer
        train_tax = JSONLinesDataset("/root/finetune/data/train/rulings.json", tokenizer)
        val_tax = JSONLinesDataset("/root/finetune/data/train/rulings.json", tokenizer)
        train_irs = JSONLinesDataset("/root/finetune/data/train/advice_memos.json", tokenizer)
        val_irs = JSONLinesDataset("/root/finetune/data/train/advice_memos.json", tokenizer)

        # Merge tax and IRS datasets for training & validation
        self.train_dataset = ConcatDataset([train_tax, train_irs])
        self.val_dataset = ConcatDataset([val_tax, val_irs])

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn)


# Load model
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)


# Define Trainer
class LlamaFineTuner(pl.LightningModule):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.model.train()

    def training_step(self, batch, batch_idx):
        outputs = self.model(**batch)
        loss = outputs.loss
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        outputs = self.model(**batch)
        val_loss = outputs.loss
        self.log("val_loss", val_loss, sync_dist=True)

    def configure_optimizers(self):
        return torch.optim.AdamW(self.parameters(), lr=5e-6)


trainer = pl.Trainer(accelerator="gpu", devices=8, num_nodes=2, strategy="ddp", precision="bf16", max_epochs=1)

# Initialize DataModule
data_module = LawDataModule(batch_size=1)


@rank_zero_only
def save_model(model, tokenizer, output_dir="/root/finetune/saved_model"):
    """ Saves model only on rank 0 to avoid multiple copies. """
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"Fine-tuned model and tokenizer saved to {output_dir}")
# Train


if __name__ == "__main__":
    fine_tuner = LlamaFineTuner(model)
    trainer.fit(fine_tuner, data_module)
    save_model(fine_tuner.model, tokenizer)

